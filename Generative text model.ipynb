{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import pipeline, set_seed\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\nimport torch\n\n# Optional: fix the seed for reproducibility\nset_seed(42)\n\n# Step 1: Dropdown for selecting GPT model\nmodel_selector = widgets.Dropdown(\n    options=[\n        ('Distilled GPT-2', 'distilgpt2'),\n        ('GPT-2 Large', 'gpt2-large'),\n        ('GPT-Neo 1.3B', 'EleutherAI/gpt-neo-1.3B'),\n        ('GPT-Neo 2.7B', 'EleutherAI/gpt-neo-2.7B'),\n        ('GPT-J 6B', 'EleutherAI/gpt-j-6B'),\n        ('Falcon-7B Instruct (High RAM)', 'tiiuae/falcon-7b-instruct')\n    ],\n    description='Select Model:',\n    value='EleutherAI/gpt-neo-1.3B',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='60%')\n)\n\n# Step 2: Input text box for prompt/topic\nuser_input = widgets.Text(\n    value='The future of AI is',\n    placeholder='Type your topic here...',\n    description='Input Topic:',\n    style={'description_width': 'initial'},\n    layout=widgets.Layout(width='80%')\n)\n\n# Step 3: Generate button\ngenerate_button = widgets.Button(\n    description=\"Generate Text\",\n    button_style='success'\n)\n\n# Step 4: Output area\noutput_area = widgets.Output()\n\n# Step 5: Generate text function\ndef generate_text(_):\n    output_area.clear_output()\n    prompt = user_input.value.strip()\n    model_id = model_selector.value\n    \n    if not prompt:\n        with output_area:\n            display(Markdown(\"‚ö†Ô∏è **Please enter a topic first.**\"))\n        return\n    \n    with output_area:\n        try:\n            # Prefer GPU if available\n            device = 0 if torch.cuda.is_available() else -1\n            print(f\"üöÄ Loading model `{model_id}` on {'CUDA' if device == 0 else 'CPU'}...\")\n            \n            generator = pipeline(\"text-generation\", model=model_id, device=device)\n            result = generator(\n                prompt,\n                max_length=250,\n                do_sample=True,\n                temperature=0.8,\n                top_k=50,\n                top_p=0.95,\n                repetition_penalty=1.5,  # Stronger repetition control\n                truncation=True,\n                pad_token_id=50256\n            )[0][\"generated_text\"]\n            \n            # Remove incomplete last sentence\n            def remove_incomplete_sentence(text):\n                # Split by common sentence endings\n                sentences = []\n                current_sentence = \"\"\n                \n                for char in text:\n                    current_sentence += char\n                    if char in '.!?':\n                        sentences.append(current_sentence.strip())\n                        current_sentence = \"\"\n                \n                # If there's remaining text that doesn't end with punctuation, it's incomplete\n                if current_sentence.strip() and not current_sentence.strip()[-1] in '.!?':\n                    # Don't add the incomplete sentence\n                    pass\n                else:\n                    # Add the last sentence if it's complete\n                    if current_sentence.strip():\n                        sentences.append(current_sentence.strip())\n                \n                return ' '.join(sentences)\n            \n            # Clean the result\n            cleaned_result = remove_incomplete_sentence(result)\n            \n            # Display the cleaned generated text\n            display(Markdown(f\"### üìù Generated Text:\\n\\n{cleaned_result}\"))\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e).lower():\n                display(Markdown(\"‚ùå **CUDA Out of Memory!** Try switching to a smaller model.\"))\n            else:\n                display(Markdown(f\"‚ö†Ô∏è **Error:** {str(e)}\"))\n\ngenerate_button.on_click(generate_text)\n\n# Step 6: Display all components\ndisplay(model_selector, user_input, generate_button, output_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T05:58:03.396674Z","iopub.execute_input":"2025-06-26T05:58:03.396921Z","iopub.status.idle":"2025-06-26T05:58:30.335396Z","shell.execute_reply.started":"2025-06-26T05:58:03.396903Z","shell.execute_reply":"2025-06-26T05:58:30.334590Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 05:58:13.389036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750917493.606398      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750917493.662507      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dropdown(description='Select Model:', index=2, layout=Layout(width='60%'), options=(('Distilled GPT-2', 'disti‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b217217c83341dfa923b23110b47436"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Text(value='The future of AI is', description='Input Topic:', layout=Layout(width='80%'), placeholder='Type yo‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e05d67d9f49744f8a67bb6a16e36cf86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Button(button_style='success', description='Generate Text', style=ButtonStyle())","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae40b2457e04fb6ac7b1c31ca9fc2d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28625eb75ff24bd895c2be0340f80500"}},"metadata":{}}],"execution_count":1}]}